---
title: "Future Ethics"
author: "Cennydd Bowles"
last_interaction: "Mittwoch, 16. Juli 2025 22:41:37"
---


Build, measure, learn, repeat.

Bringing up ethics in the workplace often prompts two objections. First, some people claim ethics doesn’t belong in industry, and that acceptable behaviour is for the market or the law to decide.

The second common objection to business ethics is that it will hamper innovation.

# Trouble in paradise

# Do no harm?

## Unintended consequences and externalities

When you invent the ship, you also invent the shipwreck; when you invent the plane you also invent the plane crash; and when you invent electricity, you invent electrocution... Every technology carries its own negativity, which is invented at the same time as technical progress.

A cousin of the unintended consequence is the externality. An externality is the economist’s label for Someone Else’s Problem, an effect that falls on someone outside the system.

## Algorithmic bias

Verbeek’s mediation theory tells us we shouldn’t separate technological and human action, technologies will mirror social biases by default

A relativist argues there’s no one moral truth, no lone guiding star for behaviour; instead, ethical rules depend on social differences and vary across cultures.

Globalisation is particularly challenging for relativism. Should we accept another society’s choices that we find repellent? Should we do business with countries that actively discriminate, or where corruption is widespread? Moral relativism suggests a free-for-all: who are we to argue with the norms of another culture?

Ad absurdum, if goodness is in the eye of the beholder, slave owners get to decide whether slavery is ethical.

## The technocracy trap

Given how heavily technology shapes modern culture, this means technologists have significant influence over social norms: ethical decisions that should be democratic are instead technocratic.

Society sees classifiers like race and, increasingly, gender as spectrums rather than discrete buckets; our algorithms should too.

Economic historian Carlota Perez argues that every technological revolution follows a tight script.18 First, an ‘irruption’ phase, when a technology showing both promise and threat attracts heavy speculative investment. Then ‘frenzy’, a period of intense exploration, in which new markets explode into life and companies often cut ethical corners to make a quick buck. Eventually the bubble bursts and high-profile failures force regulators to step in. The momentum swings from finance to production as the technology becomes widespread; a phase of ‘synergy’. Finally the revolution is complete and ‘maturity’ dominates. The market is saturated, awaiting the next disruption.

Ethical conventions don’t themselves solve ethical problems:

Codes can offer some structure to ethical debate, but are usually too vague to resolve it.

Consider two well-known maxims: the bioethics pledge ‘First, do no harm’ and Google’s famous ‘Don’t be evil’. Although pithy, both statements are awkwardly imprecise in practice. What is harm? What is evil? Who decides?

If you live near a Whole Foods, if no one in your family serves in the military, if you’re paid by the year, not the hour, if most people you know finished college, if no one you know uses meth, if you married once and remain married, if you’re not one of 65 million Americans with a criminal record — if any or all of these things describe you, then accept the possibility that actually, you may not know what’s going on and you may be part of the problem. —Anand Giridharadas28

# Chapter 3 Persuasive mechanisms

Hypertext therefore tends to break apart centralised, linear narratives and encourages instead apophenia, a habit of imposing relationships on unconnected things.

Furries, flat-earthers, and fascists alike can stitch together their own narratives from the digital fragments of the web, and broadcast them into their communities.

Perhaps the biggest ethical challenge of hypernudging is its invisibility. There’s no way to tell whether a camera is feeding a persuasion engine; soon we won’t know whether a help desk is staffed by humans or hypernudging algorithms.

## Justifying persuasion: folk ethics

The weakest justification for persuasion – or, indeed, anything else – is that everyone’s at it. This is a classic ethical trap,

Our competitors’ moral choices are irrelevant to our own.

The golden rule – do as you would be done by – is more helpful.

The golden rule’s biggest flaw is its egocentrism.

The golden rule’s biggest flaw is its egocentrism. It encourages everyone to see themselves as the ideal ethical arbiter, whether their interests align with others’ or not.

Perhaps we should instead treat others how they would like to be treated: the platinum rule.

But simply asking people what their best interests are has its flaws too: people’s stated opinions are unreliable, and at times everyone contradicts their own interests by seeking out things that limit their capacity to thrive, like tobacco and alcohol.

## Persuasive theories



Social design researcher Nynke Tromp suggests we classify persuasion by strength and visibility, creating four types of influence: decisive, coercive, persuasive, and seductive.

## The role of intent

intent – the why behind the design. Intent comes up often in ethics, and is the cornerstone of the principle of double effect, which states that harm is sometimes acceptable as a side effect of doing good.

‘there should always be a good and clear reason for why the nudge will improve the welfare of those being nudged.’

Focusing just on intent also allows us to wriggle off the hook of unintended consequences.

## Introducing deontology

Immanuel Kant, a pioneer of deontological thought, proposed a powerful idea: when faced with an ethical choice, we should universalise our thinking.

What if everyone did what I’m about to do?

## Ethical experimentation

## The veil of ignorance

Rawls contends that society – for our purposes we’ll extend this to technological society too – is best structured as if its architects didn’t know their eventual role in the system. Beneath a veil of ignorance, we wouldn’t know our social status, our intelligence, or even our interests; but if the system is fair we should be satisfied wherever we ended up.

It’s the you-cut-I-choose cake-sharing protocol from your childhood, stretched over an entire population.

Applied to persuasion, the veil of ignorance suggests we should only create persuasive systems that would be fair to the persuader and persuaded alike.

## Better persuasion

We can also select mutually destructive targets, metrics chosen in pairs such that one will suffer if we simply game the other. For example, dark patterns may well extract more revenue per user, but they’ll also harm retention if users feel duped. Choosing both revenue and retention as mutually destructive targets provides a minor safeguard against abuse; if both measures move in the right direction, we can be confident things are genuinely improving.

## Regulation and opt-out

# The data deluge

## Data beyond advertising

## Raw data is an oxymoron

## Resigned to insecurity

Privacy, then, is a hand on the dial, not just a padlock.

## The value exchange in practice

## Introducing utilitarianism

Utilitarians base ethical decisions on a simple but powerful question – am I maximising happiness for the greatest number of people? – and, by extension, am I minimising pain?

## Utilitarianism or deontology?

The choice looks quite different from the two ethical perspectives. Utilitarians emphasise the consequences – pretty minor in this case – while deontologists are more concerned by the nature of the act itself and its contradiction of moral duty.

There are some parallels with web ad blocking today, a debate beset with hyperbole. One side claims ad tech is so hostile it’s effectively malware; the ad industry counters by claiming blocking is theft:

When I hear ‘is it ethical to build AI applications that do X on some massively aggregated database’, I want to ask whether it was ethical to massively aggregate that database in the first place. —Matt Blaze27

Rawls’s veil of ignorance can be useful here. To design fair systems we should pretend we don’t know the role we’ll assume in the system. Would we be as happy with the user’s end of the bargain as the advertiser’s end? What if we were a user from an underrepresented group, or one who has previously suffered data discrimination and redlining?

## Self-ownership and pocket AI

Privacy fights tend to require taking a side.

## The nuclear no-data option

## Privacy as strategy

# Seeing through new eyes

## ‘If I don’t do it, someone else will’

Randy Cohen, former author of NYT column ‘The Ethicist’, is scathing of the ‘If I don’t…’ defence. ‘“If I don’t do it, someone else will” [does] not justify nefarious conduct. Someone else will do pretty much anything. I’ve met “someone else,” and he’s quite the little weasel.’

## The deadly seams

## The trolley problem is a red herring

## The social contract

To stave off this dystopia, we enter into a wordless pledge with one another; what we now call the social contract. We agree to submit to forms of authority such as governments, monarchies, or courts, and trade some of our freedoms for security and social stability. For Hobbes, this contract is the basis of political legitimacy, the alternative being chaos and amorality.

## Introducing virtue ethics

Virtue ethics, the third major pillar of modern ethics, revives these ancient ideas. To a virtue ethicist, to live well – to flourish – we must demonstrate positive virtues in all our choices

While deontologists focus on duty, and utilitarians look only at consequences, virtue ethicists are more concerned by overall moral character.

As an entry point to thinking about virtue, we can apply another ethical test: would I be happy for my decision to appear on the front page of tomorrow’s news?

# Chapter 6 You have twenty seconds to comply

Karl Popper’s paradox of tolerance illustrates the danger of inaction. When we fail to act against the intolerant, we give them power which, unchecked, may cause the destruction of tolerant people. Excessive tolerance threatens tolerance itself.

## Post-privacy

We live, therefore, in an era of surveillance realism, in which we disapprove of bulk data collection but support giving governments more powers to fight terrorism.

‘Nothing to hide, nothing to fear’ is a common defence in privacy debates, arguing that surveillance only has trivial impact on law-abiding citizens.

But the deepest flaw with ‘nothing to hide’ is it assumes authorities are infallible. The defence only works if technology is always accurate and no one is ever wrongly detained or convicted.

there’s a strong moral case that privacy helps us lead lives of eudaemonia and dignity; we might even argue that privacy is necessary for trust in society and democracy.

# Software is heating the world

In total, 70% of global internet traffic is routed through Loudoun County, Virginia, where data centres are almost entirely powered by dirty, nonrenewable sources.

It’s helpful to look at the tech industry’s environmental impact across three categories: data centres, networks, and devices themselves.

# No cash, no jobs, no hope

# A new tech philosophy

## Beware the business case

The business case is the ethical problem. A moral argument that hinges entirely on financial consequences is unwittingly agreeing that ethics is subservient to profit;

‘Reward the right behaviors and you will get the right results. Reward only the results and you will get all sorts of behaviors.’
